{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JPeU_Sg28FG5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load text\n",
        "with open('./Bowie.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read().lower()\n",
        "\n",
        "# Tokenize text into words\n",
        "#tokens = text.split()\n",
        "import re\n",
        "tokens = re.findall(r'\\b\\w+\\b|[\\n]', text)\n",
        "\n",
        "unique_words = sorted(set(tokens))\n",
        "word_to_index = {w: i for i, w in enumerate(unique_words)}\n",
        "index_to_word = {i: w for w, i in word_to_index.items()}\n",
        "vocab_size = len(unique_words)\n",
        "\n",
        "# Create sequences\n",
        "seq_length = 10\n",
        "sequences = []\n",
        "for i in range(seq_length, len(tokens)):\n",
        "    seq = tokens[i - seq_length:i + 1]\n",
        "    sequences.append([word_to_index[word] for word in seq])\n",
        "\n",
        "# Split sequences into X and y\n",
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:, :-1], sequences[:, -1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "9aVqE1ZM8Gxm",
        "outputId": "603e38f5-2f4c-48bf-be3e-8b05546fec80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 50)          307600    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 150)               120600    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6152)              928952    \n",
            "=================================================================\n",
            "Total params: 1,357,152\n",
            "Trainable params: 1,357,152\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=50))\n",
        "model.add(LSTM(150, return_sequences=False))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iluMrNWU8lCT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1791/1791 [==============================] - 7s 4ms/step - loss: 5.5805 - accuracy: 0.1613\n",
            "Epoch 2/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 4.5128 - accuracy: 0.2439\n",
            "Epoch 3/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 3.7932 - accuracy: 0.3124\n",
            "Epoch 4/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 3.2650 - accuracy: 0.3816\n",
            "Epoch 5/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 2.8466 - accuracy: 0.4497\n",
            "Epoch 6/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 2.5093 - accuracy: 0.5081\n",
            "Epoch 7/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 2.2374 - accuracy: 0.5587\n",
            "Epoch 8/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 2.0116 - accuracy: 0.6021\n",
            "Epoch 9/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 1.8219 - accuracy: 0.6382\n",
            "Epoch 10/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 1.6594 - accuracy: 0.6693\n",
            "Epoch 11/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 1.5182 - accuracy: 0.6981\n",
            "Epoch 12/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 1.3957 - accuracy: 0.7219\n",
            "Epoch 13/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 1.2909 - accuracy: 0.7423\n",
            "Epoch 14/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 1.1962 - accuracy: 0.7609\n",
            "Epoch 15/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 1.1122 - accuracy: 0.7774\n",
            "Epoch 16/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 1.0366 - accuracy: 0.7922\n",
            "Epoch 17/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.9697 - accuracy: 0.8056\n",
            "Epoch 18/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.9093 - accuracy: 0.8168\n",
            "Epoch 19/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.8547 - accuracy: 0.8271\n",
            "Epoch 20/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.8052 - accuracy: 0.8369\n",
            "Epoch 21/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.7580 - accuracy: 0.8461\n",
            "Epoch 22/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.7178 - accuracy: 0.8533\n",
            "Epoch 23/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.6802 - accuracy: 0.8609\n",
            "Epoch 24/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.6464 - accuracy: 0.8671\n",
            "Epoch 25/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.6156 - accuracy: 0.8719\n",
            "Epoch 26/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.5842 - accuracy: 0.8786\n",
            "Epoch 27/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.5588 - accuracy: 0.8832\n",
            "Epoch 28/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.5331 - accuracy: 0.8879\n",
            "Epoch 29/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.5097 - accuracy: 0.8923\n",
            "Epoch 30/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.4882 - accuracy: 0.8967\n",
            "Epoch 31/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.4683 - accuracy: 0.9006\n",
            "Epoch 32/50\n",
            "1791/1791 [==============================] - 9s 5ms/step - loss: 0.4507 - accuracy: 0.9038\n",
            "Epoch 33/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.4331 - accuracy: 0.9071\n",
            "Epoch 34/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.4178 - accuracy: 0.9102\n",
            "Epoch 35/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.4018 - accuracy: 0.9135\n",
            "Epoch 36/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.3871 - accuracy: 0.9162\n",
            "Epoch 37/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.3762 - accuracy: 0.9174\n",
            "Epoch 38/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.3632 - accuracy: 0.9196\n",
            "Epoch 39/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.3521 - accuracy: 0.9222\n",
            "Epoch 40/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.3395 - accuracy: 0.9245\n",
            "Epoch 41/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.3278 - accuracy: 0.9271\n",
            "Epoch 42/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.3203 - accuracy: 0.9279\n",
            "Epoch 43/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.3095 - accuracy: 0.9306\n",
            "Epoch 44/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.3035 - accuracy: 0.9309\n",
            "Epoch 45/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.2939 - accuracy: 0.9330\n",
            "Epoch 46/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.2864 - accuracy: 0.9345\n",
            "Epoch 47/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.2799 - accuracy: 0.9354\n",
            "Epoch 48/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.2716 - accuracy: 0.9375\n",
            "Epoch 49/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.2663 - accuracy: 0.9380\n",
            "Epoch 50/50\n",
            "1791/1791 [==============================] - 6s 3ms/step - loss: 0.2592 - accuracy: 0.9398\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x2521463eb48>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X, y, epochs=50, batch_size=128)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "t1ZV7b4i8Mqy"
      },
      "outputs": [],
      "source": [
        "def generate_text(seed_text, n_words=50):\n",
        "    result = seed_text.lower().split()\n",
        "    for _ in range(n_words):\n",
        "        encoded = [word_to_index.get(w, 0) for w in result[-seq_length:]]\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        pred = model.predict(encoded, verbose=0)\n",
        "        next_index = np.argmax(pred)\n",
        "        next_word = index_to_word[next_index]\n",
        "        result.append(next_word)\n",
        "    return ' '.join(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save('bowie_word_generator.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load('bowie_word_generator.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9D24e-IF8Pv7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the stars look very different today with hell \n",
            " \n",
            " don t have no cool \n",
            " when a better take a little \n",
            " wants a little toy \n",
            " she says she is all right all \n",
            " here would have my world on any other word \n",
            " sell me a coat with buttons of love \n",
            " you need some easy baby s all a vast creation \n",
            " trying to go away \n",
            " \n",
            " still don t ask the sun from the crowd \n",
            " i m in the world said i can do \n",
            " don t you try \n",
            " i keep forgetting you don t want to do me \n",
            " you know i m a mess \n",
            " i m a fire of my \n",
            " for your heart \n",
            " i m not alone \n",
            " \n",
            " better i m looking \n",
            " and i m gonna buy a good friend that i want \n",
            " it s got to be writ now it s as so as they say \n",
            " i said just to long \n",
            " just walking the wall out can t come \n",
            " and she says shh \n",
            " she says \n",
            " oh she s all right \n",
            " i think she s all i ve been my side \n",
            " the turn\n"
          ]
        }
      ],
      "source": [
        "seed = \"the stars look very different today\"\n",
        "print(generate_text(seed, n_words=200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
